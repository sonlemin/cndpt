# src/utils.py
import time, random, re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlsplit, urlunsplit

def clean_url(u: str) -> str:
    """B·ªè query/utm ƒë·ªÉ tr√°nh tr√πng link"""
    parts = urlsplit(str(u))
    return urlunsplit((parts.scheme, parts.netloc, parts.path, "", ""))

def html_to_text(html: str) -> str:
    """Chuy·ªÉn HTML -> text th√¥"""
    soup = BeautifulSoup(html, "lxml")
    return " ".join(soup.get_text(" ", strip=True).split())

def fetch_html(session: requests.Session, url: str, headers=None, retry=2, timeout=20) -> str:
    for _ in range(retry):
        r = session.get(url, headers=headers, timeout=timeout)
        if r.status_code == 429:
            time.sleep(random.uniform(40, 70))  # b·ªã ch·∫∑n
            continue
        r.raise_for_status()
        return r.text
    return ""

def sleep_random(a=1.0, b=2.0):
    time.sleep(random.uniform(a, b))

def normalize_spaces(s: str) -> str:
    return " ".join(str(s).strip().split())

def clean_text_vn(text: str) -> str:
    text = str(text).lower()
    text = re.sub(r"http\S+", " ", text)                 # b·ªè link
    text = re.sub(r"[^\w√Ä-·ªπ\s\+\#\.\-]", " ", text)      # gi·ªØ ch·ªØ VN + k√Ω t·ª± k·ªπ thu·∫≠t
    text = re.sub(r"\s+", " ", text).strip()
    return text
"""
utils.py

C√°c h√†m ti·ªán √≠ch d√πng chung cho to√†n b·ªô project.

Bao g·ªìm:
- Network utilities: fetch_html, sleep_random
- Text processing: clean_text, normalize_spaces, html_to_text
- URL handling: clean_url
- Data validation: validate_dataframe
"""

import re
import time
import random
from urllib.parse import urlsplit, urlunsplit
from typing import Optional, Tuple
import requests
from requests.exceptions import Timeout, ConnectionError, HTTPError
from bs4 import BeautifulSoup
import pandas as pd


def clean_url(url: str) -> str:
    """
    Lo·∫°i b·ªè query parameters v√† fragments kh·ªèi URL.
    
    T·∫†I SAO:
    - URL c√≥ query params kh√°c nhau nh∆∞ng tr·ªè v·ªÅ c√πng 1 page
    - VD: /job-123?utm_source=facebook vs /job-123?utm_source=google
    - C·∫ßn chu·∫©n h√≥a ƒë·ªÉ tr√°nh duplicate
    
    S·ª¨ D·ª§NG ·ªû ƒê√ÇU:
    - 03_preprocess_clean.py: Deduplicate links
    
    V√ç D·ª§:
        Input:  "https://topcv.vn/job-123?utm_source=fb&ref=home#section1"
        Output: "https://topcv.vn/job-123"
    
    Args:
        url: URL c·∫ßn clean
    
    Returns:
        URL ƒë√£ lo·∫°i b·ªè query params v√† fragments
    """
    parts = urlsplit(url)
    # Keep scheme, netloc, path only
    # Drop query and fragment
    clean_parts = (parts.scheme, parts.netloc, parts.path, '', '')
    return urlunsplit(clean_parts)


def normalize_spaces(text: str) -> str:
    """
    Chu·∫©n h√≥a whitespace: lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a.
    
    T·∫†I SAO:
    - HTML th∆∞·ªùng c√≥ nhi·ªÅu spaces, tabs, newlines th·ª´a
    - "python    django" v√† "python django" n√™n ƒë∆∞·ª£c coi l√† gi·ªëng nhau
    
    S·ª¨ D·ª§NG ·ªû ƒê√ÇU:
    - html_to_text(): Sau khi extract text
    - clean_text_vn(): Trong qu√° tr√¨nh clean
    
    V√ç D·ª§:
        Input:  "python    django\n\npostgresql"
        Output: "python django postgresql"
    
    Args:
        text: Text c·∫ßn normalize
    
    Returns:
        Text v·ªõi single spaces
    """
    # Split by any whitespace, then join with single space
    return ' '.join(text.split())


def html_to_text(html: str) -> str:
    """
    Convert HTML sang plain text.
    
    T·∫†I SAO:
    - C·∫ßn extract text content t·ª´ HTML response
    - Lo·∫°i b·ªè tags, scripts, styles
    
    C√ÅCH HO·∫†T ƒê·ªòNG:
    1. Parse HTML v·ªõi BeautifulSoup
    2. Remove <script> v√† <style> tags
    3. Extract text
    4. Normalize spaces
    
    S·ª¨ D·ª§NG ·ªû ƒê√ÇU:
    - 02_scrape_detail_topcv.py: Parse job content
    
    V√ç D·ª§:
        Input:  "<div>Job: <b>Python</b> Developer</div><script>alert('hi')</script>"
        Output: "Job: Python Developer"
    
    Args:
        html: HTML string
    
    Returns:
        Plain text content
    """
    try:
        soup = BeautifulSoup(html, 'lxml')
        
        # Remove script and style elements
        for tag in soup(['script', 'style', 'noscript']):
            tag.decompose()
        
        # Get text
        text = soup.get_text(separator=' ', strip=True)
        
        # Normalize spaces
        text = normalize_spaces(text)
        
        return text
    except Exception as e:
        print(f"‚ö†Ô∏è Error parsing HTML: {e}")
        return ""


def fetch_html(
    session: requests.Session,
    url: str,
    headers: Optional[dict] = None,
    retry: int = 3,
    timeout: int = 30
) -> Optional[str]:
    """
    Fetch HTML t·ª´ URL v·ªõi retry mechanism.
    
    T·∫†I SAO C·∫¶N RETRY:
    - Network kh√¥ng ·ªïn ƒë·ªãnh (timeout, connection reset)
    - Server overload (503)
    - Rate limiting (429)
    
    CHI·∫æN L∆Ø·ª¢C RETRY:
    - Timeout/ConnectionError: Retry ngay v·ªõi exponential backoff
    - 429 (Rate limit): Sleep 40-70s r·ªìi retry
    - 404/other errors: Kh√¥ng retry (v√¥ √≠ch)
    
    S·ª¨ D·ª§NG ·ªû ƒê√ÇU:
    - 01_scrape_list_topcv.py: Fetch search result pages
    - 02_scrape_detail_topcv.py: Fetch job detail pages
    
    V√ç D·ª§:
        session = requests.Session()
        html = fetch_html(session, "https://topcv.vn/job-123")
        if html:
            content = html_to_text(html)
    
    Args:
        session: requests.Session object (for connection reuse)
        url: URL to fetch
        headers: Optional headers dict
        retry: Number of retry attempts (default: 3)
        timeout: Timeout in seconds (default: 30)
    
    Returns:
        HTML string if success, None if failed
    """
    for attempt in range(1, retry + 1):
        try:
            response = session.get(
                url,
                headers=headers,
                timeout=timeout,
                allow_redirects=True
            )
            
            # Handle rate limiting
            if response.status_code == 429:
                if attempt < retry:
                    sleep_time = random.uniform(40, 70)
                    print(f"‚ö†Ô∏è Rate limited (429). Sleeping {sleep_time:.0f}s...")
                    time.sleep(sleep_time)
                    continue
                else:
                    print(f"‚ùå Rate limited after {retry} retries")
                    return None
            
            # Raise for bad status codes
            response.raise_for_status()
            
            # Success
            return response.text
            
        except Timeout:
            if attempt < retry:
                sleep_time = 5 * attempt  # Exponential backoff
                print(f"‚è±Ô∏è  Timeout (attempt {attempt}/{retry}). Retrying in {sleep_time}s...")
                time.sleep(sleep_time)
                continue
            else:
                print(f"‚ùå Timeout after {retry} retries")
                return None
                
        except ConnectionError:
            if attempt < retry:
                sleep_time = 5 * attempt
                print(f"üîå Connection error (attempt {attempt}/{retry}). Retrying in {sleep_time}s...")
                time.sleep(sleep_time)
                continue
            else:
                print(f"‚ùå Connection error after {retry} retries")
                return None
                
        except HTTPError as e:
            # Don't retry for 404, 403, etc
            print(f"‚ùå HTTP error {e.response.status_code}: {url}")
            return None
            
        except Exception as e:
            print(f"‚ùå Unexpected error: {type(e).__name__}: {e}")
            return None
    
    return None


def sleep_random(min_seconds: float, max_seconds: float) -> None:
    """
    Sleep random time trong kho·∫£ng [min, max].
    
    T·∫†I SAO C·∫¶N RANDOM:
    - Fixed sleep pattern ‚Üí d·ªÖ b·ªã detect l√† bot
    - Random pattern ‚Üí gi·ªëng human behavior h∆°n
    
    S·ª¨ D·ª§NG ·ªû ƒê√ÇU:
    - 01_scrape_list_topcv.py: Gi·ªØa c√°c pages
    - 02_scrape_detail_topcv.py: Gi·ªØa c√°c jobs
    
    V√ç D·ª§:
        sleep_random(2.0, 5.0)  # Sleep 2-5 gi√¢y
    
    Args:
        min_seconds: Minimum sleep time
        max_seconds: Maximum sleep time
    """
    sleep_time = random.uniform(min_seconds, max_seconds)
    time.sleep(sleep_time)


def clean_text_vn(text: str) -> str:
    """
    Clean text ti·∫øng Vi·ªát: lowercase, lo·∫°i b·ªè URLs, emails, special chars.
    
    T·∫†I SAO:
    - Chu·∫©n h√≥a ƒë·ªÉ d·ªÖ so s√°nh v√† search
    - Lo·∫°i b·ªè noise (URLs, emails kh√¥ng c·∫ßn thi·∫øt)
    - Preserve technical terms (C++, C#, .NET)
    
    C√ÅCH HO·∫†T ƒê·ªòNG:
    1. Lowercase
    2. Preserve C++, C#, .NET (technical terms)
    3. Remove URLs
    4. Remove emails
    5. Normalize spaces
    
    S·ª¨ D·ª§NG ·ªû ƒê√ÇU:
    - 03_preprocess_clean.py: Clean title v√† content
    
    V√ç D·ª§:
        Input:  "Senior C++ Developer\nEmail: hr@company.com\nhttps://company.com"
        Output: "senior c++ developer"
    
    Args:
        text: Text c·∫ßn clean
    
    Returns:
        Cleaned text
    """
    if pd.isna(text) or not isinstance(text, str):
        return ""
    
    # Lowercase
    text = text.lower()
    
    # Preserve technical terms before cleaning
    # Replace v·ªõi placeholders
    text = text.replace('c++', '__CPP__')
    text = text.replace('c#', '__CSHARP__')
    text = text.replace('.net', '__DOTNET__')
    
    # Remove URLs
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    
    # Remove emails
    text = re.sub(r'\S+@\S+', '', text)
    
    # Remove special characters (keep alphanumeric, Vietnamese chars, spaces)
    # Keep: a-z, 0-9, Vietnamese chars, spaces, hyphens
    text = re.sub(r'[^\w\s√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë\-]', ' ', text)
    
    # Restore technical terms
    text = text.replace('__cpp__', 'c++')
    text = text.replace('__csharp__', 'c#')
    text = text.replace('__dotnet__', '.net')
    
    # Normalize spaces
    text = normalize_spaces(text)
    
    return text


def validate_dataframe(
    df: pd.DataFrame,
    required_columns: list,
    name: str = "DataFrame"
) -> None:
    """
    Validate DataFrame c√≥ ƒë·ªß columns c·∫ßn thi·∫øt.
    
    T·∫†I SAO:
    - Early detection of data issues
    - Clear error messages
    - Prevent cryptic errors downstream
    
    S·ª¨ D·ª§NG ·ªû ƒê√ÇU:
    - ƒê·∫ßu m·ªói script: Validate input data
    
    V√ç D·ª§:
        df = pd.read_csv('data.csv')
        validate_dataframe(df, ['title', 'link', 'content'], 'Job data')
    
    Args:
        df: DataFrame to validate
        required_columns: List of required column names
        name: Name for error messages
    
    Raises:
        ValueError: If missing required columns
    """
    missing = set(required_columns) - set(df.columns)
    if missing:
        raise ValueError(
            f"{name} thi·∫øu columns: {missing}\n"
            f"C√≥: {list(df.columns)}\n"
            f"C·∫ßn: {required_columns}"
        )


def extract_salary_range(text: str) -> Tuple[Optional[float], Optional[float], Optional[float]]:
    """
    Extract salary range t·ª´ text (VND ho·∫∑c USD).
    
    PATTERNS SUPPORTED:
    1. "15-25 tri·ªáu" ‚Üí (15.0, 25.0, 20.0)
    2. "20 tri·ªáu" ‚Üí (20.0, 20.0, 20.0)
    3. "1000-1500 USD" ‚Üí (23.0, 34.5, 28.75) - convert to VND tri·ªáu
    4. "th·ªèa thu·∫≠n" ‚Üí (None, None, None)
    
    CONVERSION RATE:
    - 1 USD ‚âà 23,000 VND = 0.023 tri·ªáu VND
    
    S·ª¨ D·ª§NG ·ªû ƒê√ÇU:
    - 04_extract_features.py: Extract salary from job descriptions
    
    V√ç D·ª§:
        text = "L∆∞∆°ng: 15-25 tri·ªáu VND"
        min_sal, max_sal, avg_sal = extract_salary_range(text)
        # ‚Üí (15.0, 25.0, 20.0)
    
    Args:
        text: Text ch·ª©a salary info
    
    Returns:
        Tuple of (min_salary, max_salary, avg_salary) in tri·ªáu VND
        Returns (None, None, None) if not found or "th·ªèa thu·∫≠n"
    """
    if pd.isna(text) or not isinstance(text, str):
        return None, None, None
    
    text = text.lower()
    
    # Check for "th·ªèa thu·∫≠n" / "ÌòëÏùò" / "negotiable"
    if re.search(r'th·ªèa thu·∫≠n|tho·∫£ thu·∫≠n|ÂçîË≠∞|negotiable|competitive', text):
        return None, None, None
    
    # Try USD first (convert to tri·ªáu VND)
    # Pattern: "1000-1500 USD" ho·∫∑c "1000 USD"
    usd_pattern = r'(\d+)\s*[-~]\s*(\d+)\s*(?:usd|\$)'
    match = re.search(usd_pattern, text)
    if match:
        min_usd = float(match.group(1))
        max_usd = float(match.group(2))
        # 1 USD ‚âà 23,000 VND = 0.023 tri·ªáu VND
        min_vnd = min_usd * 0.023
        max_vnd = max_usd * 0.023
        avg_vnd = (min_vnd + max_vnd) / 2
        return min_vnd, max_vnd, avg_vnd
    
    # Single USD value
    usd_single = r'(\d+)\s*(?:usd|\$)'
    match = re.search(usd_single, text)
    if match:
        usd = float(match.group(1))
        vnd = usd * 0.023
        return vnd, vnd, vnd
    
    # VND range: "15-25 tri·ªáu" or "15~25tr"
    vnd_range = r'(\d+(?:\.\d+)?)\s*[-~]\s*(\d+(?:\.\d+)?)\s*(?:tri·ªáu|tr|trieu|million)'
    match = re.search(vnd_range, text)
    if match:
        min_sal = float(match.group(1))
        max_sal = float(match.group(2))
        avg_sal = (min_sal + max_sal) / 2
        return min_sal, max_sal, avg_sal
    
    # Single VND value: "20 tri·ªáu"
    vnd_single = r'(\d+(?:\.\d+)?)\s*(?:tri·ªáu|tr|trieu|million)'
    match = re.search(vnd_single, text)
    if match:
        sal = float(match.group(1))
        return sal, sal, sal
    
    return None, None, None


def extract_experience_years(text: str) -> Optional[float]:
    """
    Extract s·ªë nƒÉm kinh nghi·ªám y√™u c·∫ßu t·ª´ text.
    
    PATTERNS SUPPORTED:
    1. "3 nƒÉm kinh nghi·ªám" ‚Üí 3.0
    2. "kinh nghi·ªám 2-3 nƒÉm" ‚Üí 2.5 (average)
    3. "fresher" / "kh√¥ng y√™u c·∫ßu" ‚Üí 0.0
    4. "5+ years experience" ‚Üí 5.0
    
    S·ª¨ D·ª§NG ·ªû ƒê√ÇU:
    - 04_extract_features.py: Extract experience requirement
    
    V√ç D·ª§:
        text = "Y√™u c·∫ßu: 3 nƒÉm kinh nghi·ªám Python"
        years = extract_experience_years(text)
        # ‚Üí 3.0
    
    Args:
        text: Text ch·ª©a experience info
    
    Returns:
        S·ªë nƒÉm kinh nghi·ªám (float), None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    if pd.isna(text) or not isinstance(text, str):
        return None
    
    text = text.lower()
    
    # Check for fresher / no experience required
    if re.search(r'fresher|kh√¥ng y√™u c·∫ßu kinh nghi·ªám|no experience|entry level', text):
        return 0.0
    
    # Pattern 1: "X nƒÉm" or "X years"
    pattern1 = r'(\d+)\s*(?:nƒÉm|years?|yr|yrs)'
    matches = re.findall(pattern1, text)
    if matches:
        # Take the first occurrence
        return float(matches[0])
    
    # Pattern 2: Range "X-Y nƒÉm" ‚Üí average
    pattern2 = r'(\d+)\s*[-~]\s*(\d+)\s*(?:nƒÉm|years?)'
    match = re.search(pattern2, text)
    if match:
        min_exp = float(match.group(1))
        max_exp = float(match.group(2))
        return (min_exp + max_exp) / 2
    
    # Pattern 3: "X+ nƒÉm"
    pattern3 = r'(\d+)\+\s*(?:nƒÉm|years?)'
    match = re.search(pattern3, text)
    if match:
        return float(match.group(1))
    
    return None


def save_checkpoint(data, filepath: str, mode: str = 'dataframe') -> None:
    """
    Save checkpoint ƒë·ªÉ tr√°nh m·∫•t data khi crash.
    
    T·∫†I SAO:
    - Scraping l√¢u (2-3 gi·ªù) ‚Üí r·ªßi ro crash cao
    - Checkpoint m·ªói 10 items ‚Üí m·∫•t t·ªëi ƒëa 10 items khi crash
    
    S·ª¨ D·ª§NG ·ªû ƒê√ÇU:
    - 01_scrape_list_topcv.py: Save m·ªói 10 pages
    - 02_scrape_detail_topcv.py: Save m·ªói 10 jobs
    
    V√ç D·ª§:
        rows = []
        for i, item in enumerate(items):
            rows.append(process(item))
            if (i + 1) % 10 == 0:
                save_checkpoint(rows, 'data.csv')
    
    Args:
        data: Data to save (DataFrame or list of dicts)
        filepath: Path to save file
        mode: 'dataframe' or 'list'
    """
    try:
        if mode == 'dataframe':
            if isinstance(data, pd.DataFrame):
                data.to_csv(filepath, index=False, encoding='utf-8-sig')
            else:
                pd.DataFrame(data).to_csv(filepath, index=False, encoding='utf-8-sig')
        elif mode == 'list':
            pd.DataFrame(data).to_csv(filepath, index=False, encoding='utf-8-sig')
        
        print(f"üíæ Checkpoint saved: {len(data)} rows")
    except Exception as e:
        print(f"‚ö†Ô∏è Error saving checkpoint: {e}")


def format_duration(seconds: float) -> str:
    """
    Format duration t·ª´ seconds sang human-readable string.
    
    V√ç D·ª§:
        format_duration(3665) ‚Üí "1h 1m 5s"
        format_duration(125) ‚Üí "2m 5s"
        format_duration(45) ‚Üí "45s"
    
    Args:
        seconds: Duration in seconds
    
    Returns:
        Formatted string
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    
    parts = []
    if hours > 0:
        parts.append(f"{hours}h")
    if minutes > 0:
        parts.append(f"{minutes}m")
    if secs > 0 or not parts:  # Always show seconds if no other units
        parts.append(f"{secs}s")
    
    return " ".join(parts)


if __name__ == "__main__":
    # Test functions
    print("Testing utils functions...")
    
    # Test clean_url
    url = "https://topcv.vn/job-123?utm_source=fb&ref=home#section"
    print(f"\nclean_url:")
    print(f"  Input:  {url}")
    print(f"  Output: {clean_url(url)}")
    
    # Test normalize_spaces
    text = "python    django\n\npostgresql"
    print(f"\nnormalize_spaces:")
    print(f"  Input:  {repr(text)}")
    print(f"  Output: {repr(normalize_spaces(text))}")
    
    # Test clean_text_vn
    text = "Senior C++ Developer\nEmail: hr@company.com"
    print(f"\nclean_text_vn:")
    print(f"  Input:  {text}")
    print(f"  Output: {clean_text_vn(text)}")
    
    # Test extract_salary_range
    texts = [
        "L∆∞∆°ng: 15-25 tri·ªáu VND",
        "Salary: 1000-1500 USD",
        "L∆∞∆°ng th·ªèa thu·∫≠n",
        "20 tri·ªáu"
    ]
    print(f"\nextract_salary_range:")
    for t in texts:
        result = extract_salary_range(t)
        print(f"  {t:30s} ‚Üí {result}")
    
    # Test extract_experience_years
    texts = [
        "Y√™u c·∫ßu 3 nƒÉm kinh nghi·ªám",
        "Fresher welcome",
        "5+ years of experience",
        "2-3 nƒÉm"
    ]
    print(f"\nextract_experience_years:")
    for t in texts:
        result = extract_experience_years(t)
        print(f"  {t:30s} ‚Üí {result}")
    
    # Test format_duration
    durations = [45, 125, 3665, 7325]
    print(f"\nformat_duration:")
    for d in durations:
        print(f"  {d:5d}s ‚Üí {format_duration(d)}")
    
    print("\n‚úÖ All tests completed!")